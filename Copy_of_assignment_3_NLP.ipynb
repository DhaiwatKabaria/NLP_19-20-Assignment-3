{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of assignment 3_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhaiwatKabaria/NLP_19-20-Assignment-3/blob/master/Copy_of_assignment_3_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4jTkm3N7TBU",
        "colab_type": "text"
      },
      "source": [
        "## **DATA IMPORT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmItylUYWkkB",
        "colab_type": "code",
        "outputId": "7e1978fa-616a-4d1a-bfd0-f886b660a449",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kyx-lYL8WvUk",
        "colab_type": "code",
        "outputId": "e814e449-024f-4376-e1c7-aa7434b211f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls gdrive/My\\ Drive/Colab\\ Notebooks/Assignment\\ 3/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Copy of assignment 3_NLP.ipynb'   test.txt   train.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQgo11YD7ZSF",
        "colab_type": "text"
      },
      "source": [
        "# **Engaging TPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jug-NSNvmegW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "23263004-e773-4381-9cd8-14094919a0e7"
      },
      "source": [
        "\n",
        "import os\n",
        "import pprint\n",
        "import tensorflow as tf\n",
        "\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
        "else:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address is', tpu_address)\n",
        "\n",
        "  with tf.Session(tpu_address) as session:\n",
        "    devices = session.list_devices()\n",
        "    \n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(devices)"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.124.155.58:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 11842671269238085405),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7192888418168673641),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 154666549457836629),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 10554125658587804241),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 3052941084770963049),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 15470580659476450457),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 14037756964735731413),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 12410204332054980515),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 17394413644234911272),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 3785491663331170356),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 3952758901492480569)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fixm78_pm3jo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten,Input\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAlmqE7Mnh1i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4954fbdf-041d-4310-b56a-46eb9540071f"
      },
      "source": [
        "print(tf.__version__)\n",
        "print(tf.keras.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n",
            "2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUeYwSro7fnL",
        "colab_type": "text"
      },
      "source": [
        "### **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCpdNJpdqGOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fp = open(\"/content/gdrive/My Drive/Colab Notebooks/Assignment 3/train.txt\", 'r')\n",
        "final_raw_train=[]\n",
        "q=0\n",
        "while(q!=15131):\n",
        "  lst=[]\n",
        "  q=q+1\n",
        "  for line in fp:\n",
        "    line = line.strip()\n",
        "    if len(line.split())>1:\n",
        "      lst.append(line.split())\n",
        "    if line=='':\n",
        "      final_raw_train.append(lst)\n",
        "      break\n",
        "fp = open(\"/content/gdrive/My Drive/Colab Notebooks/Assignment 3/test.txt\", 'r')\n",
        "final_raw_test=[]\n",
        "q=0\n",
        "while(q!=1869):\n",
        "  lst=[]\n",
        "  q=q+1\n",
        "  for line in fp:\n",
        "    line = line.strip()\n",
        "    if len(line.split())>1:\n",
        "      lst.append(line.split())\n",
        "    if line=='':\n",
        "      final_raw_test.append(lst)\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8Usd0wrpHQ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "a44b4ea8-ad35-4081-996e-a9b1ecfbf056"
      },
      "source": [
        "final_new_train[1]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['madarchod', 'Hin'],\n",
              " ['mulle', 'Hin'],\n",
              " ['ye', 'Hin'],\n",
              " ['mathura', 'Hin'],\n",
              " ['me', 'Hin'],\n",
              " ['nahi', 'Hin'],\n",
              " ['dikha', 'Hin'],\n",
              " ['tha', 'Hin'],\n",
              " ['jab', 'Hin'],\n",
              " ['mullo', 'Hin'],\n",
              " ['ne', 'Hin'],\n",
              " ['hindu', 'Hin'],\n",
              " ['ko', 'Hin'],\n",
              " ['iss', 'Hin'],\n",
              " ['liye', 'Hin'],\n",
              " ['mara', 'Hin'],\n",
              " ['ki', 'Hin'],\n",
              " ['vo', 'Hin'],\n",
              " ['lasse', 'Hin'],\n",
              " ['ki', 'Hin'],\n",
              " ['paise', 'Hin'],\n",
              " ['mag', 'Hin'],\n",
              " ['liye', 'Hin'],\n",
              " ['the', 'Eng']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAKrLUdC_Q2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = []\n",
        "train_id = []\n",
        "for tweet in final_raw_train:\n",
        "  train_id.append(tweet[0][1])\n",
        "  y_train.append(tweet[0][2])\n",
        "\n",
        "y_test = []\n",
        "test_id = []\n",
        "for tweet in final_raw_test:\n",
        "  test_id.append(tweet[0][1])\n",
        "  y_test.append(tweet[0][2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLR3Tqf_otc5",
        "colab_type": "code",
        "outputId": "22fba99e-0a4d-4335-d7ee-1adb6741baf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from collections import Counter\n",
        "y_train_dist = Counter(y_train)\n",
        "y_train_dist"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'negative': 4459, 'positive': 5034, 'neutral': 5638})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE8HEd7gXQBX",
        "colab_type": "code",
        "outputId": "fa4c2e20-b70c-4b10-fc90-eec69bca4a39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from collections import Counter\n",
        "y_test_dist = Counter(y_test)\n",
        "y_test_dist"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'neutral': 754, 'negative': 533, 'positive': 582})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb1bylRj_l3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"y_train.txt\",'w') as f:\n",
        "  for id1, sentiment in zip(train_id, y_train):\n",
        "      f.write(id1+\"\\t\"+sentiment+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDIMDfdhXaL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"y_test.txt\",'w') as f:\n",
        "  for id1, sentiment in zip(test_id, y_test):\n",
        "      f.write(id1+\"\\t\"+sentiment+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpTVvdNkPDOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_dct = {'negative':-1, \"neutral\":0,\"positive\":1}\n",
        "y_train_final = []\n",
        "for i, stance in enumerate(y_train):\n",
        "  y_train_final.append(y_dct[stance])\n",
        "\n",
        "import numpy as np \n",
        "y_train_final = np.array(y_train_final)\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "enc.fit(y_train_final.reshape(-1, 1))\n",
        "y_train_final = enc.transform(y_train_final.reshape(-1, 1)).toarray()\n",
        "#########################################################################################################################\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAzkOJpfaGU2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('ytest',y_test_final)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvDJS6qkZ4ui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_dct = {'negative':-1, \"neutral\":0,\"positive\":1}\n",
        "\n",
        "y_test_final = []\n",
        "for i, stance in enumerate(y_test):\n",
        "  y_test_final.append(y_dct[stance])\n",
        "\n",
        "import numpy as np \n",
        "\n",
        "y_test_final = np.array(y_test_final)\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "enc.fit(y_test_final.reshape(-1, 1))\n",
        "y_test_final = enc.transform(y_test_final.reshape(-1, 1)).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LalbiyQpuHkZ",
        "colab_type": "code",
        "outputId": "65616cdb-33c4-4eee-af04-d579540a50c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "y_test_final"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       ...,\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0q24AfHyBtm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "final_new_train = []\n",
        "for i in range(15131):\n",
        "  temp = []\n",
        "  for j in range(len(final_raw_train[i])):\n",
        "    if bool(re.match(\"^[a-zA-Z]+$\",final_raw_train[i][j][0])):\n",
        "      if bool(re.match(\"^(http)|(^t$)|(^co$)\",final_raw_train[i][j][0])):\n",
        "        continue\n",
        "      else:\n",
        "        final_raw_train[i][j][0] = final_raw_train[i][j][0].lower()\n",
        "        temp.append(final_raw_train[i][j])\n",
        "  final_new_train.append(temp[1:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIW8VS_eapvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "final_new_test = []\n",
        "for i in range(1869):\n",
        "  temp = []\n",
        "  for j in range(len(final_raw_test[i])):\n",
        "    if bool(re.match(\"^[a-zA-Z]+$\",final_raw_test[i][j][0])):\n",
        "      if bool(re.match(\"^(http)|(^t$)|(^co$)\",final_raw_test[i][j][0])):\n",
        "        continue\n",
        "      else:\n",
        "        final_raw_test[i][j][0] = final_raw_test[i][j][0].lower()\n",
        "        temp.append(final_raw_test[i][j])\n",
        "  final_new_test.append(temp[1:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig4kO9mlgYS1",
        "colab_type": "code",
        "outputId": "d88e6498-4c6e-4af9-c3bb-72d58ce9f607",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(final_new_test[2])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXs0ztKPvoPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"X_train.txt\",'w') as f:\n",
        "  for id1, tweet in zip(train_id, final_new_train):\n",
        "      f.write(id1+\"\\t\"+\" \".join([j[0] for j in tweet[1:-1]])+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T96-y6X9nOqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"X_test.txt\",'w') as f:\n",
        "  for id1, tweet in zip(test_id, final_new_test):\n",
        "      f.write(id1+\"\\t\"+\" \".join([j[0] for j in tweet[1:-1]])+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUID1Cy-3Q7l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "01f06a41-0298-43ef-8348-bae723feee41"
      },
      "source": [
        "final_new_train[:5]"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[['adilnisarbutt', 'Hin'],\n",
              "  ['pakistan', 'Hin'],\n",
              "  ['ka', 'Hin'],\n",
              "  ['ghra', 'Hin'],\n",
              "  ['tauq', 'Hin'],\n",
              "  ['he', 'Eng'],\n",
              "  ['pakistan', 'Eng'],\n",
              "  ['israel', 'Eng'],\n",
              "  ['ko', 'Eng'],\n",
              "  ['tasleem', 'Hin'],\n",
              "  ['nahein', 'Hin'],\n",
              "  ['kerta', 'Hin'],\n",
              "  ['isko', 'Hin'],\n",
              "  ['palestine', 'Hin'],\n",
              "  ['kehta', 'Hin'],\n",
              "  ['he', 'Hin'],\n",
              "  ['occupied', 'Hin'],\n",
              "  ['palestine', 'Hin']],\n",
              " [['madarchod', 'Hin'],\n",
              "  ['mulle', 'Hin'],\n",
              "  ['ye', 'Hin'],\n",
              "  ['mathura', 'Hin'],\n",
              "  ['me', 'Hin'],\n",
              "  ['nahi', 'Hin'],\n",
              "  ['dikha', 'Hin'],\n",
              "  ['tha', 'Hin'],\n",
              "  ['jab', 'Hin'],\n",
              "  ['mullo', 'Hin'],\n",
              "  ['ne', 'Hin'],\n",
              "  ['hindu', 'Hin'],\n",
              "  ['ko', 'Hin'],\n",
              "  ['iss', 'Hin'],\n",
              "  ['liye', 'Hin'],\n",
              "  ['mara', 'Hin'],\n",
              "  ['ki', 'Hin'],\n",
              "  ['vo', 'Hin'],\n",
              "  ['lasse', 'Hin'],\n",
              "  ['ki', 'Hin'],\n",
              "  ['paise', 'Hin'],\n",
              "  ['mag', 'Hin'],\n",
              "  ['liye', 'Hin'],\n",
              "  ['the', 'Eng']],\n",
              " [['narendramodi', 'Hin'],\n",
              "  ['manya', 'Hin'],\n",
              "  ['pradhan', 'Hin'],\n",
              "  ['mantri', 'Hin'],\n",
              "  ['mahoday', 'Hin'],\n",
              "  ['shriman', 'Hin'],\n",
              "  ['narendra', 'Hin'],\n",
              "  ['modi', 'Hin'],\n",
              "  ['ji', 'Hin'],\n",
              "  ['pradhanmantri', 'Hin'],\n",
              "  ['banne', 'Hin'],\n",
              "  ['par', 'Hin'],\n",
              "  ['hardik', 'Hin'],\n",
              "  ['badhai', 'Hin'],\n",
              "  ['tahe', 'Hin'],\n",
              "  ['dil', 'Hin']],\n",
              " [['atheist', 'Eng'],\n",
              "  ['krishna', 'Eng'],\n",
              "  ['jcb', 'Eng'],\n",
              "  ['full', 'Eng'],\n",
              "  ['trend', 'Eng'],\n",
              "  ['me', 'Eng'],\n",
              "  ['chal', 'Hin'],\n",
              "  ['rahi', 'Hin'],\n",
              "  ['aa', 'Hin']],\n",
              " [['abhisharsharma', 'Hin'],\n",
              "  ['ravishkumarblog', 'Hin'],\n",
              "  ['loksabha', 'Eng'],\n",
              "  ['me', 'Hin'],\n",
              "  ['janta', 'Hin'],\n",
              "  ['sirf', 'Hin'],\n",
              "  ['modi', 'Hin'],\n",
              "  ['ko', 'Hin'],\n",
              "  ['vote', 'Hin'],\n",
              "  ['de', 'Hin'],\n",
              "  ['rahi', 'Hin'],\n",
              "  ['thi', 'Hin'],\n",
              "  ['na', 'Hin'],\n",
              "  ['ki', 'Hin'],\n",
              "  ['kisi', 'Hin'],\n",
              "  ['mp', 'Hin'],\n",
              "  ['or', 'Eng'],\n",
              "  ['bjp', 'Eng'],\n",
              "  ['ko', 'Eng'],\n",
              "  ['without', 'Eng'],\n",
              "  ['m', 'Eng']]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1kAN3yIESQ5",
        "colab_type": "code",
        "outputId": "7d3f29c1-3846-42d6-9ca9-3a630db06e12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install flair"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flair\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/22/8fc8e5978ec05b710216735ca47415700e83f304dec7e4281d61cefb6831/flair-0.4.4-py3-none-any.whl (193kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 20.0MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 61kB 3.0MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 71kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 81kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 92kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 153kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 163kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 174kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 184kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 3.4MB/s \n",
            "\u001b[?25hCollecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/1c/c757b93147a219cf1e25cef7e1ad9b595b7f802159493c45ce116521caff/sqlitedict-1.6.0.tar.gz\n",
            "Collecting tiny-tokenizer[all]\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/ee/08078f68165a7465f028f3505e6a749b50f6f5c229bd272a863ab07acdc2/tiny_tokenizer-3.0.1.tar.gz\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.5)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from flair) (3.9.0)\n",
            "Collecting ipython==7.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/2c/c7d44277b599df35af734d8f4142d501192fdb7aef5d04daf882d7eccfbc/ipython-7.6.1-py3-none-any.whl (774kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 56.0MB/s \n",
            "\u001b[?25hCollecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/59/4bc44158a767a6d66de18c4136c8aa90491d56cc951c10b74dd1e13213c9/langdetect-1.0.7.zip (998kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 52.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.4)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.28.1)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/59/6ed78856ab99d2da04084b59e7da797972baa0efecb71546b16d48e49d9b/segtok-1.5.7.tar.gz\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
            "Requirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.6/dist-packages (from flair) (0.2.0)\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 49.5MB/s \n",
            "\u001b[?25hCollecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/f6/89/62912e01f3cede11edcc0abf81298e3439d9c06c8dce644369380ed13f6d/Deprecated-1.2.7-py2.py3-none-any.whl\n",
            "Collecting bpemb>=0.2.9\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/70/468a9652095b370f797ed37ff77e742b11565c6fd79eaeca5f2e50b164a7/bpemb-0.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from flair) (1.24.3)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.3.1+cu100)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.1.1)\n",
            "Collecting transformers>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/f9/51824e40f0a23a49eab4fcaa45c1c797cbf9761adedd0b558dab7c958b34/transformers-2.1.1-py3-none-any.whl (311kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 58.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
            "Collecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 29.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from flair) (0.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from flair) (0.4.2+cu100)\n",
            "Collecting natto-py\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/a0/eaac1ed66c02823a2423a21de863da53a5268ce77582d91d1edb45a403dc/natto-py-0.9.0.tar.gz\n",
            "Collecting kytea\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/bc/702d01a96d5d094bd9f3c2eb1d12153daf8edf7bf5d78b9a2dae1202df07/kytea-0.1.4-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 40.0MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 42.1MB/s \n",
            "\u001b[?25hCollecting SudachiPy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/09/7c55ab89d7bcdd8075929add8b096c4b5347d661d78e7d4884214f672513/SudachiPy-0.4.0-py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.15.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (2.1.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (41.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.4.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.7.5)\n",
            "Collecting prompt-toolkit<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/61/2dfea88583d5454e3a64f9308a686071d58d59a55db638268a6413e1eb6d/prompt_toolkit-2.0.10-py3-none-any.whl (340kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 61.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.7.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.3.3)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect->flair) (1.12.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (19.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.8.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (7.2.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (0.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.3.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.17.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.11.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bpemb>=0.2.9->flair) (2.21.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.6.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 50.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.0.0->flair) (1.10.14)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.9.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->flair) (0.21.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->flair) (4.3.0)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from natto-py->tiny-tokenizer[all]->flair) (1.13.2)\n",
            "Collecting dartsclone~=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/4d/45acbe9d0795d8ceef0fee1f9ac2dcbf27dca3a0578a023fcdc3fef6fd89/dartsclone-0.6.tar.gz\n",
            "Requirement already satisfied: sortedcontainers~=2.1.0 in /usr/local/lib/python3.6/dist-packages (from SudachiPy->tiny-tokenizer[all]->flair) (2.1.0)\n",
            "Requirement already satisfied: parso>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from jedi>=0.10->ipython==7.6.1->flair) (0.5.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython==7.6.1->flair) (0.1.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython==7.6.1->flair) (0.6.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (2019.9.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.0.0->flair) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.0.0->flair) (0.14.0)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.14 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (1.13.14)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (0.9.4)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision->flair) (0.46)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->natto-py->tiny-tokenizer[all]->flair) (2.19)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from dartsclone~=0.6.0->SudachiPy->tiny-tokenizer[all]->flair) (0.29.14)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.14->boto3->transformers>=2.0.0->flair) (0.15.2)\n",
            "Building wheels for collected packages: sqlitedict, tiny-tokenizer, langdetect, segtok, mpld3, natto-py, sacremoses, dartsclone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.6.0-cp36-none-any.whl size=14689 sha256=b013e97191afbce542e87bddd40306b83c43263235a15d34ee4f3f468d1b8212\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/57/d3/907c3ee02d35e66f674ad0106e61f06eeeb98f6ee66a6cc3fe\n",
            "  Building wheel for tiny-tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tiny-tokenizer: filename=tiny_tokenizer-3.0.1-cp36-none-any.whl size=9444 sha256=8670fefa133c4cafab282a36897b2641f7c952e96f8b535190b402c43d60e276\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/04/72/d04956c4b03e3b03e5e095c06cbabc9bfb6f1bec02288eacdb\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.7-cp36-none-any.whl size=993460 sha256=91af6dd9be572f58db570c44e62c874d124fea13fdfd107ec491ec0ff289e1fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/0c/a9/1647275e7ef5014e7b83ff30105180e332867d65e7617ddafe\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.7-cp36-none-any.whl size=23258 sha256=741a1942c8d6599b4d2a1c279e7d53043131ca8e172732942b901022262e188d\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/ee/a8/6112173f1386d33eebedb3f73429cfa41a4c3084556bcee254\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116679 sha256=3a2ce88e6369c40b53da44633fc86be67512761194aae766975ec5040b81b1c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for natto-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for natto-py: filename=natto_py-0.9.0-cp36-none-any.whl size=45075 sha256=b4a370de9d0d46125a1b89f8b307f024cca9ed8c7487695e2902012a8fa160ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/98/3a/ebfb1636e18698b3f47d8caa3f90fc3a91f1ea58430616018f\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=e313c67da4a72e53acf3a7bc768de870fdd693a487e0f3ff5bb93499c79764a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "  Building wheel for dartsclone (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dartsclone: filename=dartsclone-0.6-cp36-cp36m-linux_x86_64.whl size=413250 sha256=d35140648c96cdf049cbd76a6f5cf7f22d4175a23ef9144c16d6a0268136a7a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/cd/70/fe43307bf7398243155108f4f5a258ef336923d65ec4af93cd\n",
            "Successfully built sqlitedict tiny-tokenizer langdetect segtok mpld3 natto-py sacremoses dartsclone\n",
            "\u001b[31mERROR: jupyter-console 5.2.0 has requirement prompt-toolkit<2.0.0,>=1.0.0, but you'll have prompt-toolkit 2.0.10 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 7.6.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sqlitedict, natto-py, kytea, sentencepiece, dartsclone, SudachiPy, tiny-tokenizer, prompt-toolkit, ipython, langdetect, regex, segtok, deprecated, bpemb, sacremoses, transformers, mpld3, flair\n",
            "  Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "Successfully installed SudachiPy-0.4.0 bpemb-0.3.0 dartsclone-0.6 deprecated-1.2.7 flair-0.4.4 ipython-7.6.1 kytea-0.1.4 langdetect-1.0.7 mpld3-0.3 natto-py-0.9.0 prompt-toolkit-2.0.10 regex-2019.11.1 sacremoses-0.0.35 segtok-1.5.7 sentencepiece-0.1.83 sqlitedict-1.6.0 tiny-tokenizer-3.0.1 transformers-2.1.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ep1eE6-G762",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.data import Sentence\n",
        "\n",
        "english_sent_train = []\n",
        "for i in range(len(final_new_train)):\n",
        "  sent = Sentence(\" \".join([j[0] for j in final_new_train[i]]))\n",
        "  english_sent_train.append(sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UdB00EIb2Hv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.data import Sentence\n",
        "\n",
        "english_sent_test = []\n",
        "for i in range(len(final_new_test)):\n",
        "  sent = Sentence(\" \".join([j[0] for j in final_new_test[i]]))\n",
        "  english_sent_test.append(sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUhIizdX2U-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm-q_pd4HRdh",
        "colab_type": "code",
        "outputId": "5dff248c-6aea-4e93-bbe7-076aa7789f49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentence: \"min of lyching manakgupta officeofknath mein kahna nae chahta qki mere yaha btay tco\" - 14 Tokens"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7iSiLG6tl2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPZAqOKG0FUN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        },
        "outputId": "53c2a31f-46cc-4a43-97b9-171c42b34544"
      },
      "source": [
        "%pip install tensorflow==1.13.1"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
            "\u001b[K     |████████████████████████████████| 92.5MB 27kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 41.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.33.6)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.2.2)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 43.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.17.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (41.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.1)\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed mock-3.0.5 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2E94ftLlEp0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "781a86e9-abd3-4e65-f77b-75c3ad121a70"
      },
      "source": [
        "X_train.cpu().numpy().shape"
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15131, 31, 3072)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiWgYWGf7vVM",
        "colab_type": "text"
      },
      "source": [
        "# TRYING TO MAKE TPU MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly1ViYXHfkTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_model(batch_size=None):\n",
        "  source = Input(shape=(31,), batch_size=batch_size, dtype=tf.int32, name='Input')\n",
        "  embedding = Embedding(input_dim=3072, output_dim=128, name='Embedding')(source)\n",
        "  lstm = Bidirectional(LSTM(32, name = 'LSTM'), name='Bidirectional')(embedding)\n",
        "  #lstm = LSTM(32, name = 'LSTM')(embedding)\n",
        "  predicted_var = Dense(1, activation='sigmoid', name='Output')(lstm)\n",
        "  model = tf.keras.Model(inputs=[source], outputs=[predicted_var])\n",
        "  model.compile(\n",
        "      optimizer=tf.train.RMSPropOptimizer(learning_rate=0.01),\n",
        "      loss='binary_crossentropy',\n",
        "      metrics=['acc'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjmMSsEgixzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wNZZleqCo0G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "669fe440-90c8-4939-8876-bdec53e5d1ac"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "training_model = make_model()\n",
        "training_model.summary()"
      ],
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-275-b31859446212>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtraining_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-274-a4191f8aa99a>\u001b[0m in \u001b[0;36mmake_model\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m#lstm = Bidirectional(LSTM(32, name = 'LSTM'), input_shape=3072, name='Bidirectional')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mpredicted_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Output'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicted_var\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   model.compile(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;31m# Build layer if applicable (if the `build` method has been overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[0;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m     input_spec.assert_input_compatibility(\n\u001b[0;32m-> 1591\u001b[0;31m         self.input_spec, inputs, self.name)\n\u001b[0m\u001b[1;32m   1592\u001b[0m     \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         spec.max_ndim is not None):\n\u001b[0;32m--> 109\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[1;32m    111\u001b[0m                          \u001b[0mlayer_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' is incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'LSTM' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P4HmZf2x0R9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "072b4403-aa90-4959-a84b-ac12cdab9ced"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "# This address identifies the TPU we'll use when configuring TensorFlow.\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    training_model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))"
      ],
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.124.155.58:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 11842671269238085405)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7192888418168673641)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 154666549457836629)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 10554125658587804241)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 3052941084770963049)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 15470580659476450457)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 14037756964735731413)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 12410204332054980515)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 17394413644234911272)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 3785491663331170356)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 3952758901492480569)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q83LS6VLfOld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iYieAAjR78u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "60bbee67-dca4-45fe-a94f-63cbd15094c8"
      },
      "source": [
        "history = tpu_model.fit(X_train.cpu().numpy(), y_train_final,\n",
        "                        epochs=20,\n",
        "                        batch_size=35 * 8,\n",
        "                        validation_split=0.2)\n",
        "tpu_model.save_weights('./tpu_model.h5', overwrite=True)\n",
        "tpu_model.evaluate(X_test.cpu().numpy(), y_test_final, batch_size=35 * 8)"
      ],
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-263-56a8aa1b766c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m35\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                         validation_split=0.2)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtpu_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./tpu_model.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtpu_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m35\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m                                   \u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m                                   \u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m                                   steps_per_epoch, validation_steps, **kwargs)\n\u001b[0m\u001b[1;32m   1533\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_to_infeed_manager_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1611\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steps_per_epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1613\u001b[0;31m         validation_split=validation_split)\n\u001b[0m\u001b[1;32m   1614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m     \u001b[0;31m# Prepare validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[1;32m   2380\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m         exception_prefix='input')\n\u001b[0m\u001b[1;32m   2383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    351\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected Input to have 2 dimensions, but got array with shape (15131, 31, 3072)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kSgAWNT70WL",
        "colab_type": "text"
      },
      "source": [
        "## **CREATING EMBEDDINGS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBTe6l_1G83D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.embeddings import BertEmbeddings\n",
        "embedding_bert = BertEmbeddings(\"bert-base-cased\")\n",
        "\n",
        "embeddings_train = []\n",
        "for sent in english_sent_train:\n",
        "  embeddings_train.append(embedding_bert.embed(sent))\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "bert_embeddings_train = []\n",
        "for i in embeddings_train:\n",
        "  temp = []\n",
        "  for j in i[0]:\n",
        "    temp.append(np.array(j.embedding.cpu()))\n",
        "  bert_embeddings_train.append(np.array(temp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKxjNMt0I-rX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.loaf(\"bert_test.npy\",bert_embeddings_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwMv866QcE4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.embeddings import BertEmbeddings\n",
        "embedding_bert = BertEmbeddings(\"bert-base-cased\")\n",
        "\n",
        "embeddings_test = []\n",
        "for sent in english_sent_test:\n",
        "  embeddings_test.append(embedding_bert.embed(sent))\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "bert_embeddings_test = []\n",
        "for i in embeddings_test:\n",
        "  temp = []\n",
        "  for j in i[0]:\n",
        "    temp.append(np.array(j.embedding.cpu()))\n",
        "  bert_embeddings_test.append(np.array(temp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxNOSDslqJQ5",
        "colab_type": "code",
        "outputId": "617a1d28-9948-4b4d-bb0c-9fa1cf9972a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1athAGyMLUTU",
        "colab_type": "code",
        "outputId": "800b91ce-bdf2-4ca9-fff9-f7595bb18fed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "bert_embeddings_train[1].shape\n"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24, 3072)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBoB2mBiRfZQ",
        "colab_type": "code",
        "outputId": "0033320c-abe0-4913-dba1-eee3034cb207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        }
      },
      "source": [
        "!pip install keras_Self_attention\n",
        "!pip install keras_multi_head\n",
        "!pip install keras_tqdm"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_Self_attention\n",
            "  Downloading https://files.pythonhosted.org/packages/44/3e/eb1a7c7545eede073ceda2f5d78442b6cad33b5b750d7f0742866907c34b/keras-self-attention-0.42.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras_Self_attention) (1.17.4)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras_Self_attention) (2.2.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras_Self_attention) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_Self_attention) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_Self_attention) (1.3.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_Self_attention) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras_Self_attention) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_Self_attention) (1.0.8)\n",
            "Building wheels for collected packages: keras-Self-attention\n",
            "  Building wheel for keras-Self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-Self-attention: filename=keras_self_attention-0.42.0-cp36-none-any.whl size=17296 sha256=055e0c9421e0443208b107aed7ccfc878045fd47df11b189f46f698d389f523b\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/05/a0/99c0cf60d383f0494e10eca2b238ea98faca9a1fe03cac2894\n",
            "Successfully built keras-Self-attention\n",
            "Installing collected packages: keras-Self-attention\n",
            "Successfully installed keras-Self-attention-0.42.0\n",
            "Collecting keras_multi_head\n",
            "  Downloading https://files.pythonhosted.org/packages/40/3e/d0a64bb2ac5217928effe4507c26bbd19b86145d16a1948bc2d4f4c6338a/keras-multi-head-0.22.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras_multi_head) (1.17.4)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras_multi_head) (2.2.5)\n",
            "Collecting keras-self-attention==0.41.0\n",
            "  Downloading https://files.pythonhosted.org/packages/1b/1c/01599219bef7266fa43b3316e4f55bcb487734d3bafdc60ffd564f3cfe29/keras-self-attention-0.41.0.tar.gz\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras_multi_head) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_multi_head) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_multi_head) (1.3.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras_multi_head) (2.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_multi_head) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_multi_head) (1.12.0)\n",
            "Building wheels for collected packages: keras-multi-head, keras-self-attention\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.22.0-cp36-none-any.whl size=15371 sha256=6a95d76127a6f68c67cc8f3834dbab8f49c5b7561cbf6bb66d6cc0987fbd953a\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/df/3f/81b36f41b66e6a9cd69224c70a737de2bb6b2f7feb3272c25e\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.41.0-cp36-none-any.whl size=17290 sha256=ac74cf385c1b9b992ba4d1cd4675e2d591e8834c336d40e53b7ebc67d4e93d67\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/dc/17/84258b27a04cd38ac91998abe148203720ca696186635db694\n",
            "Successfully built keras-multi-head keras-self-attention\n",
            "Installing collected packages: keras-self-attention, keras-multi-head\n",
            "  Found existing installation: keras-self-attention 0.42.0\n",
            "    Uninstalling keras-self-attention-0.42.0:\n",
            "      Successfully uninstalled keras-self-attention-0.42.0\n",
            "Successfully installed keras-multi-head-0.22.0 keras-self-attention-0.41.0\n",
            "Collecting keras_tqdm\n",
            "  Downloading https://files.pythonhosted.org/packages/16/5c/ac63c65b79a895b8994474de2ad4d5b66ac0796b8903d60cfea3f8308d5c/keras_tqdm-2.0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras_tqdm) (2.2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from keras_tqdm) (4.28.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_tqdm) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_tqdm) (1.3.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_tqdm) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras_tqdm) (2.8.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_tqdm) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras_tqdm) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_tqdm) (1.17.4)\n",
            "Installing collected packages: keras-tqdm\n",
            "Successfully installed keras-tqdm-2.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c_EWaZ3RXIV",
        "colab_type": "code",
        "outputId": "630caf82-0efd-4268-bd1a-b57aa9e1d619",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pickle\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.nn.utils.rnn import PackedSequence, pack_padded_sequence, pad_sequence\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import GRU\n",
        "from keras.layers import Bidirectional\n",
        "from keras import regularizers\n",
        "from keras import optimizers\n",
        "from keras.layers import merge, Multiply\n",
        "from keras.layers.core import *\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import *\n",
        "from keras.layers import concatenate\n",
        "import keras\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.layers import Dropout\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "from keras_multi_head import MultiHead\n",
        "from keras_multi_head import MultiHeadAttention\n",
        "from keras_tqdm import TQDMNotebookCallback\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import regularizers\n",
        "from keras import backend as K"
      ],
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFkZQFUQu32O",
        "colab_type": "code",
        "outputId": "abbcf596-41ad-4e7c-e909-9b506d7935bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "MAX_SEQ_LEN_train = max([len(tweet) for tweet in final_new_train])\n",
        "MAX_SEQ_LEN_train"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_rANc-jfDJV",
        "colab_type": "code",
        "outputId": "e6b16cea-388e-4463-cef7-32757b77706c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "MAX_SEQ_LEN_test = max([len(tweet) for tweet in final_new_test])\n",
        "MAX_SEQ_LEN_test"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2bM0pV5NoHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(bert_embeddings, y, test_size=0.2, random_state=42, stratify=y)\n",
        "X_train = bert_embeddings_train\n",
        "X_test = bert_embeddings_test\n",
        "emb_list = []\n",
        "for i in X_train:\n",
        "  emb_list.append(torch.tensor(i[:31]))\n",
        "X_train = pad_sequence(emb_list, batch_first=True)\n",
        "\n",
        "emb_list = []\n",
        "for i in X_test:\n",
        "  emb_list.append(torch.tensor(i[:31]))\n",
        "X_test = pad_sequence(emb_list, batch_first=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H7WgvMJf12K",
        "colab_type": "code",
        "outputId": "c8fb2196-6061-44b6-d842-aabf0700058b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.cpu().numpy().shape"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15131, 31, 3072)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGdgp4AC7-tC",
        "colab_type": "text"
      },
      "source": [
        "### **BiLSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoRAcn_pRAqA",
        "colab_type": "code",
        "outputId": "17d067d4-bbb0-4d08-bc45-fd7d6bf46040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "#------------------------------------MODELS------------------------------------------#\n",
        " \n",
        "\"\"\"#### Basic BiLSTM, no appends\n",
        "**MODEL 0.0**:\n",
        "*   BERT\n",
        "*   2 LSTM layer\n",
        "*   4 dense\n",
        "*   2 dense\n",
        "\"\"\"\n",
        "\n",
        "model_Bi_LSTM_1 = Sequential()\n",
        "model_Bi_LSTM_1.add(Bidirectional(LSTM(10, dropout=0.5, recurrent_dropout=0.5, return_sequences=True), input_shape=(31, 3072), merge_mode='concat'))\n",
        "model_Bi_LSTM_1.add(Bidirectional(LSTM(10, dropout=0.5, recurrent_dropout=0.5), merge_mode='concat'))\n",
        "model_Bi_LSTM_1.add(Dense(5, activation='softmax'))\n",
        "model_Bi_LSTM_1.add(Dense(3, activation='softmax'))\n",
        "model_Bi_LSTM_1.summary()\n",
        "model_Bi_LSTM_1.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "model_Bi_LSTM_1.fit(x=X_train.cpu().numpy(), y=y_train_final, validation_data=(X_test.cpu().numpy(), y_test_final),\tbatch_size=512, epochs=15, shuffle=True)\n"
      ],
      "execution_count": 351,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_7 (Bidirection (None, 31, 20)            246640    \n",
            "_________________________________________________________________\n",
            "bidirectional_8 (Bidirection (None, 20)                2480      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 5)                 105       \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 3)                 18        \n",
            "=================================================================\n",
            "Total params: 249,243\n",
            "Trainable params: 249,243\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 15131 samples, validate on 1869 samples\n",
            "Epoch 1/15\n",
            "15131/15131 [==============================] - 28s 2ms/step - loss: 1.0916 - acc: 0.3612 - val_loss: 1.0790 - val_acc: 0.4034\n",
            "Epoch 2/15\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 1.0769 - acc: 0.3977 - val_loss: 1.0660 - val_acc: 0.4537\n",
            "Epoch 3/15\n",
            "15131/15131 [==============================] - 20s 1ms/step - loss: 1.0605 - acc: 0.4435 - val_loss: 1.0522 - val_acc: 0.4660\n",
            "Epoch 4/15\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 1.0394 - acc: 0.4821 - val_loss: 1.0363 - val_acc: 0.4708\n",
            "Epoch 5/15\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 1.0181 - acc: 0.4852 - val_loss: 1.0228 - val_acc: 0.4719\n",
            "Epoch 6/15\n",
            "15131/15131 [==============================] - 20s 1ms/step - loss: 0.9998 - acc: 0.5043 - val_loss: 1.0145 - val_acc: 0.5174\n",
            "Epoch 7/15\n",
            "15131/15131 [==============================] - 20s 1ms/step - loss: 0.9867 - acc: 0.5236 - val_loss: 1.0069 - val_acc: 0.4992\n",
            "Epoch 8/15\n",
            "15131/15131 [==============================] - 20s 1ms/step - loss: 0.9713 - acc: 0.5524 - val_loss: 0.9968 - val_acc: 0.5147\n",
            "Epoch 9/15\n",
            "15131/15131 [==============================] - 20s 1ms/step - loss: 0.9635 - acc: 0.5532 - val_loss: 0.9949 - val_acc: 0.5024\n",
            "Epoch 10/15\n",
            "15131/15131 [==============================] - 19s 1ms/step - loss: 0.9507 - acc: 0.5639 - val_loss: 0.9882 - val_acc: 0.5131\n",
            "Epoch 11/15\n",
            "15131/15131 [==============================] - 20s 1ms/step - loss: 0.9428 - acc: 0.5616 - val_loss: 0.9776 - val_acc: 0.5158\n",
            "Epoch 12/15\n",
            "15131/15131 [==============================] - 20s 1ms/step - loss: 0.9337 - acc: 0.5698 - val_loss: 0.9738 - val_acc: 0.5195\n",
            "Epoch 13/15\n",
            "15131/15131 [==============================] - 19s 1ms/step - loss: 0.9282 - acc: 0.5709 - val_loss: 0.9767 - val_acc: 0.5222\n",
            "Epoch 14/15\n",
            "15131/15131 [==============================] - 20s 1ms/step - loss: 0.9213 - acc: 0.5764 - val_loss: 0.9748 - val_acc: 0.5206\n",
            "Epoch 15/15\n",
            "15131/15131 [==============================] - 19s 1ms/step - loss: 0.9136 - acc: 0.5804 - val_loss: 0.9696 - val_acc: 0.5227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5a5b3292e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 351
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhFPwE_1pZpE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_Bi_LSTM_1.save_weights('model_Bi_LSTM_1.h5', overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAVR8x6uVX9w",
        "colab_type": "code",
        "outputId": "dd1dc0cb-4e6f-41be-d329-ee50fb8d6d9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model_Bi_LSTM_1.evaluate(X_test.cpu().numpy(), y_test_final)"
      ],
      "execution_count": 352,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1869/1869 [==============================] - 3s 1ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9696281041098891, 0.5227394329793572]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 352
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysGK5PlVkLdr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5154b0d9-71f6-460a-dabe-2f1a06a70145"
      },
      "source": [
        "type(X_test.cpu().numpy())"
      ],
      "execution_count": 385,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 385
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvoOXfsEjhmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report as clf\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "target_names=['negative','neutral','positive']\n",
        "\n",
        "df_cm = pd.DataFrame(confusion_matrix(y_test_int, y_pred), columns=target_names, index=target_names)\n",
        "df_cm.index.name = 'Actual'\n",
        "df_cm.columns.name = 'Predicted'\n",
        "print(df_cm)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy: \",accuracy_score(y_test_int, y_pred))\n",
        "print(\"\\n\",\"Report:\")\n",
        "print(clf(y_test_int, y_pred, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2_kBOLqhnKi",
        "colab_type": "text"
      },
      "source": [
        "### **BiLSTM + Self Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WbhKYsySld2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "7fd25763-9779-4430-bcad-bc83fd4212fd"
      },
      "source": [
        "\"\"\"\n",
        "#### Self Attention Library, no appends\n",
        "**MODEL 1.1 SelfAtt**\n",
        "\"\"\"\n",
        "\n",
        "model_Bi_LSTM_att1 = Sequential()\n",
        "model_Bi_LSTM_att1.add(Bidirectional(LSTM(10, dropout=0.5, recurrent_dropout=0.5, return_sequences=True), input_shape=(31, 3072), merge_mode='concat'))\n",
        "model_Bi_LSTM_att1.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "model_Bi_LSTM_att1.add(Bidirectional(LSTM(5, dropout=0.5, recurrent_dropout=0.5), merge_mode='concat'))\n",
        "#model_Bi_LSTM_att1.add(Dense(5, activation='softmax'))\n",
        "model_Bi_LSTM_att1.add(Dense(3, activation='softmax'))\n",
        "model_Bi_LSTM_att1.summary()\n",
        "model_Bi_LSTM_att1.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "model_Bi_LSTM_att1.fit(x=X_train.cpu().numpy(), y=y_train_final, validation_data=(X_test.cpu().numpy(), y_test_final),\tbatch_size=512, epochs=15, shuffle=True)\n",
        "\n"
      ],
      "execution_count": 353,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_9 (Bidirection (None, 31, 20)            246640    \n",
            "_________________________________________________________________\n",
            "seq_self_attention_8 (SeqSel (None, 31, 20)            1345      \n",
            "_________________________________________________________________\n",
            "bidirectional_10 (Bidirectio (None, 10)                1040      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 3)                 33        \n",
            "=================================================================\n",
            "Total params: 249,058\n",
            "Trainable params: 249,058\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 15131 samples, validate on 1869 samples\n",
            "Epoch 1/15\n",
            "15131/15131 [==============================] - 29s 2ms/step - loss: 1.0593 - acc: 0.4254 - val_loss: 1.0410 - val_acc: 0.4526\n",
            "Epoch 2/15\n",
            "15131/15131 [==============================] - 22s 1ms/step - loss: 1.0044 - acc: 0.4902 - val_loss: 1.0038 - val_acc: 0.4864\n",
            "Epoch 3/15\n",
            "15131/15131 [==============================] - 22s 1ms/step - loss: 0.9654 - acc: 0.5144 - val_loss: 0.9871 - val_acc: 0.4928\n",
            "Epoch 4/15\n",
            "15131/15131 [==============================] - 22s 1ms/step - loss: 0.9297 - acc: 0.5416 - val_loss: 0.9728 - val_acc: 0.5040\n",
            "Epoch 5/15\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.9127 - acc: 0.5536 - val_loss: 0.9708 - val_acc: 0.5035\n",
            "Epoch 6/15\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8996 - acc: 0.5632 - val_loss: 0.9729 - val_acc: 0.5195\n",
            "Epoch 7/15\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8902 - acc: 0.5661 - val_loss: 0.9698 - val_acc: 0.5206\n",
            "Epoch 8/15\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8797 - acc: 0.5756 - val_loss: 0.9618 - val_acc: 0.5383\n",
            "Epoch 9/15\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8654 - acc: 0.5906 - val_loss: 0.9722 - val_acc: 0.5259\n",
            "Epoch 10/15\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8619 - acc: 0.5910 - val_loss: 0.9665 - val_acc: 0.5356\n",
            "Epoch 11/15\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8508 - acc: 0.5982 - val_loss: 0.9601 - val_acc: 0.5452\n",
            "Epoch 12/15\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8448 - acc: 0.6029 - val_loss: 0.9818 - val_acc: 0.5383\n",
            "Epoch 13/15\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8407 - acc: 0.6043 - val_loss: 0.9741 - val_acc: 0.5420\n",
            "Epoch 14/15\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8325 - acc: 0.6116 - val_loss: 0.9724 - val_acc: 0.5527\n",
            "Epoch 15/15\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8240 - acc: 0.6187 - val_loss: 0.9691 - val_acc: 0.5506\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5a575d96a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 353
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsxZb1oj-dPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_Bi_LSTM_att11.save_weights('model_Bi_LSTM_att11.h5', overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-LPpdqAc-Md",
        "colab_type": "code",
        "outputId": "d27961e6-f9eb-4834-a989-f8a6363975d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model_Bi_LSTM_att1.evaluate(X_test.cpu().numpy(), y_test_final)"
      ],
      "execution_count": 354,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1869/1869 [==============================] - 3s 2ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9691216074380726, 0.550561797912265]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 354
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4I4AC_0h0T1",
        "colab_type": "text"
      },
      "source": [
        "## **GRU + Self Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxZU7oHwVen9",
        "colab_type": "code",
        "outputId": "8476cbe5-9ce0-4b3c-9fa2-5a3046577a5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "from keras.layers import GRU\n",
        "model_GRU = Sequential()\n",
        "model_GRU.add(Bidirectional(LSTM(10, dropout=0.5, recurrent_dropout=0.5, return_sequences=True), input_shape=(31, 3072), merge_mode='concat'))\n",
        "#model_GRU.add(GRU(8, dropout=0.5, recurrent_dropout=0.5, return_sequences=True))\n",
        "model_GRU.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "\n",
        "model_GRU.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "model_GRU.add(GRU(8, dropout=0.5, recurrent_dropout=0.5, return_sequences=False))\n",
        "\n",
        "#model_GRU.add(Bidirectional(LSTM(10, dropout=0.5, recurrent_dropout=0.5), merge_mode='concat'))\n",
        "#model_GRU.add(Dense(8, activation='softmax'))\n",
        "model_GRU.add(Dense(3, activation='softmax'))\n",
        "model_GRU.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "model_GRU.fit(x=X_train.cpu().numpy(), y=y_train_final, validation_data=(X_test.cpu().numpy(), y_test_final),\tbatch_size=128, epochs=16, shuffle=True)\n"
      ],
      "execution_count": 324,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 15131 samples, validate on 1869 samples\n",
            "Epoch 1/16\n",
            "15131/15131 [==============================] - 24s 2ms/step - loss: 1.0477 - acc: 0.4354 - val_loss: 1.0473 - val_acc: 0.4612\n",
            "Epoch 2/16\n",
            "15131/15131 [==============================] - 18s 1ms/step - loss: 0.9983 - acc: 0.4839 - val_loss: 1.0149 - val_acc: 0.4810\n",
            "Epoch 3/16\n",
            "15131/15131 [==============================] - 18s 1ms/step - loss: 0.9618 - acc: 0.5126 - val_loss: 1.0030 - val_acc: 0.4992\n",
            "Epoch 4/16\n",
            "15131/15131 [==============================] - 18s 1ms/step - loss: 0.9381 - acc: 0.5339 - val_loss: 0.9950 - val_acc: 0.5120\n",
            "Epoch 5/16\n",
            "15131/15131 [==============================] - 19s 1ms/step - loss: 0.9212 - acc: 0.5434 - val_loss: 0.9862 - val_acc: 0.5147\n",
            "Epoch 6/16\n",
            "15131/15131 [==============================] - 18s 1ms/step - loss: 0.9078 - acc: 0.5547 - val_loss: 0.9905 - val_acc: 0.5136\n",
            "Epoch 7/16\n",
            "15131/15131 [==============================] - 18s 1ms/step - loss: 0.8994 - acc: 0.5612 - val_loss: 0.9707 - val_acc: 0.5420\n",
            "Epoch 8/16\n",
            "15131/15131 [==============================] - 18s 1ms/step - loss: 0.8957 - acc: 0.5628 - val_loss: 0.9756 - val_acc: 0.5302\n",
            "Epoch 9/16\n",
            "15131/15131 [==============================] - 18s 1ms/step - loss: 0.8811 - acc: 0.5725 - val_loss: 0.9655 - val_acc: 0.5383\n",
            "Epoch 10/16\n",
            "15131/15131 [==============================] - 18s 1ms/step - loss: 0.8725 - acc: 0.5831 - val_loss: 0.9767 - val_acc: 0.5425\n",
            "Epoch 11/16\n",
            "15131/15131 [==============================] - 18s 1ms/step - loss: 0.8628 - acc: 0.5865 - val_loss: 0.9691 - val_acc: 0.5420\n",
            "Epoch 12/16\n",
            "15131/15131 [==============================] - 18s 1ms/step - loss: 0.8577 - acc: 0.5967 - val_loss: 0.9726 - val_acc: 0.5441\n",
            "Epoch 13/16\n",
            "15131/15131 [==============================] - 18s 1ms/step - loss: 0.8523 - acc: 0.5967 - val_loss: 0.9593 - val_acc: 0.5490\n",
            "Epoch 14/16\n",
            "15131/15131 [==============================] - 18s 1ms/step - loss: 0.8487 - acc: 0.6013 - val_loss: 0.9661 - val_acc: 0.5457\n",
            "Epoch 15/16\n",
            "15131/15131 [==============================] - 18s 1ms/step - loss: 0.8440 - acc: 0.6002 - val_loss: 0.9601 - val_acc: 0.5511\n",
            "Epoch 16/16\n",
            "15131/15131 [==============================] - 18s 1ms/step - loss: 0.8394 - acc: 0.6035 - val_loss: 0.9688 - val_acc: 0.5532\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5a8484e160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 324
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUxIYMBpLbyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ougPN3w41S9Y",
        "colab_type": "code",
        "outputId": "6a394545-8b25-4236-f7ba-0032c3c0e583",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model_GRU.evaluate(X_test.cpu().numpy(), y_test_final)"
      ],
      "execution_count": 322,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1869/1869 [==============================] - 3s 1ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9661272712223283, 0.5537720706419488]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 322
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C3hR5al2FdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_GRU.save_weights('model_GRU.h5', overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch0GYpZSh9Jb",
        "colab_type": "text"
      },
      "source": [
        "## **GRU + BiLSTM + Self Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "619IrcPBuyF4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "d6aa4f1f-b6b0-4825-9020-870cf1af06d1"
      },
      "source": [
        "from keras.layers import GRU\n",
        "\n",
        "model_Bi_LSTM_att11 = Sequential()\n",
        "model_Bi_LSTM_att11.add(Bidirectional(LSTM(10, dropout=0.5, recurrent_dropout=0.5, return_sequences=True), input_shape=(31, 3072), merge_mode='concat'))\n",
        "model_Bi_LSTM_att11.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "#model_Bi_LSTM_att11.add(Bidirectional(LSTM(5, dropout=0.5, recurrent_dropout=0.5), merge_mode='concat'))\n",
        "model_Bi_LSTM_att11.add(Bidirectional(GRU(8, dropout=0.5, recurrent_dropout=0.5, return_sequences=False), merge_mode='concat'))\n",
        "model_Bi_LSTM_att11.add(Dense(3, activation='softmax'))\n",
        "model_Bi_LSTM_att11.summary()\n",
        "model_Bi_LSTM_att11.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "model_Bi_LSTM_att11.fit(x=X_train.cpu().numpy(), y=y_train_final, validation_data=(X_test.cpu().numpy(), y_test_final),\tbatch_size=930, epochs=15, shuffle=True)\n",
        "\n"
      ],
      "execution_count": 360,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_3 (Bidirection (None, 31, 20)            246640    \n",
            "_________________________________________________________________\n",
            "seq_self_attention_2 (SeqSel (None, 31, 20)            1345      \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 16)                1392      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 249,428\n",
            "Trainable params: 249,428\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 15131 samples, validate on 1869 samples\n",
            "Epoch 1/15\n",
            "15131/15131 [==============================] - 25s 2ms/step - loss: 1.0863 - acc: 0.3884 - val_loss: 1.0629 - val_acc: 0.4585\n",
            "Epoch 2/15\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 1.0321 - acc: 0.4484 - val_loss: 1.0421 - val_acc: 0.4553\n",
            "Epoch 3/15\n",
            "15131/15131 [==============================] - 20s 1ms/step - loss: 0.9965 - acc: 0.4929 - val_loss: 1.0079 - val_acc: 0.4794\n",
            "Epoch 4/15\n",
            "15131/15131 [==============================] - 20s 1ms/step - loss: 0.9589 - acc: 0.5201 - val_loss: 0.9941 - val_acc: 0.5024\n",
            "Epoch 5/15\n",
            "15131/15131 [==============================] - 20s 1ms/step - loss: 0.9330 - acc: 0.5341 - val_loss: 0.9847 - val_acc: 0.5147\n",
            "Epoch 6/15\n",
            "15131/15131 [==============================] - 20s 1ms/step - loss: 0.9175 - acc: 0.5465 - val_loss: 0.9766 - val_acc: 0.5292\n",
            "Epoch 7/15\n",
            "15131/15131 [==============================] - 20s 1ms/step - loss: 0.9012 - acc: 0.5610 - val_loss: 0.9845 - val_acc: 0.5259\n",
            "Epoch 8/15\n",
            "15131/15131 [==============================] - 20s 1ms/step - loss: 0.8893 - acc: 0.5699 - val_loss: 0.9838 - val_acc: 0.5243\n",
            "Epoch 9/15\n",
            "15131/15131 [==============================] - 20s 1ms/step - loss: 0.8817 - acc: 0.5731 - val_loss: 0.9928 - val_acc: 0.5345\n",
            "Epoch 10/15\n",
            "15131/15131 [==============================] - 20s 1ms/step - loss: 0.8737 - acc: 0.5778 - val_loss: 0.9779 - val_acc: 0.5350\n",
            "Epoch 11/15\n",
            "15131/15131 [==============================] - 20s 1ms/step - loss: 0.8657 - acc: 0.5871 - val_loss: 0.9842 - val_acc: 0.5302\n",
            "Epoch 12/15\n",
            "15131/15131 [==============================] - 20s 1ms/step - loss: 0.8534 - acc: 0.5956 - val_loss: 0.9839 - val_acc: 0.5345\n",
            "Epoch 13/15\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8495 - acc: 0.6035 - val_loss: 0.9809 - val_acc: 0.5479\n",
            "Epoch 14/15\n",
            "15131/15131 [==============================] - 20s 1ms/step - loss: 0.8443 - acc: 0.6007 - val_loss: 0.9881 - val_acc: 0.5340\n",
            "Epoch 15/15\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8340 - acc: 0.6064 - val_loss: 0.9788 - val_acc: 0.5490\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5a4aea4240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 360
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX4jaOwX1rDs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c9ebfd26-ede2-43ac-b73b-f776df09cde9"
      },
      "source": [
        "model_Bi_LSTM_att11.evaluate(X_test.cpu().numpy(), y_test_final)"
      ],
      "execution_count": 359,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1869/1869 [==============================] - 3s 1ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0170402771779852, 0.48956661335346596]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 359
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk-6Bn4Rvbv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNVCu3kNQz7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxVIjuSsiMIz",
        "colab_type": "text"
      },
      "source": [
        "## **BiGRU + BiLSTM + Self Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3f156b7c-18d0-4250-baf5-d7e2684d1470",
        "id": "qMRH3aFAQ0Tq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "from keras.layers import GRU\n",
        "\n",
        "model_Bi_LSTM_att111 = Sequential()\n",
        "model_Bi_LSTM_att111.add(Bidirectional(LSTM(10, dropout=0.5, recurrent_dropout=0.5, return_sequences=True), input_shape=(31, 3072), merge_mode='concat'))\n",
        "model_Bi_LSTM_att111.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "#model_Bi_LSTM_att111.add(Bidirectional(LSTM(5, dropout=0.5, recurrent_dropout=0.5), merge_mode='concat'))\n",
        "model_Bi_LSTM_att111.add(Bidirectional(GRU(8, dropout=0.5, recurrent_dropout=0.5, return_sequences=False), merge_mode='concat'))\n",
        "model_Bi_LSTM_att111.add(Dense(3, activation='softmax'))\n",
        "model_Bi_LSTM_att111.summary()\n",
        "model_Bi_LSTM_att111.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "model_Bi_LSTM_att111.fit(x=X_train.cpu().numpy(), y=y_train_final, validation_data=(X_test.cpu().numpy(), y_test_final),\tbatch_size=512, epochs=15, shuffle=True)\n",
        "\n"
      ],
      "execution_count": 361,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_5 (Bidirection (None, 31, 20)            246640    \n",
            "_________________________________________________________________\n",
            "seq_self_attention_3 (SeqSel (None, 31, 20)            1345      \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection (None, 16)                1392      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 249,428\n",
            "Trainable params: 249,428\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 15131 samples, validate on 1869 samples\n",
            "Epoch 1/15\n",
            "15131/15131 [==============================] - 27s 2ms/step - loss: 1.0458 - acc: 0.4379 - val_loss: 1.0415 - val_acc: 0.4532\n",
            "Epoch 2/15\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.9812 - acc: 0.5045 - val_loss: 0.9897 - val_acc: 0.5115\n",
            "Epoch 3/15\n",
            "15131/15131 [==============================] - 22s 1ms/step - loss: 0.9342 - acc: 0.5394 - val_loss: 0.9801 - val_acc: 0.5281\n",
            "Epoch 4/15\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.9107 - acc: 0.5515 - val_loss: 0.9775 - val_acc: 0.5393\n",
            "Epoch 5/15\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8921 - acc: 0.5678 - val_loss: 0.9815 - val_acc: 0.5383\n",
            "Epoch 6/15\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8781 - acc: 0.5778 - val_loss: 0.9800 - val_acc: 0.5350\n",
            "Epoch 7/15\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8715 - acc: 0.5833 - val_loss: 0.9717 - val_acc: 0.5436\n",
            "Epoch 8/15\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8583 - acc: 0.5922 - val_loss: 0.9697 - val_acc: 0.5484\n",
            "Epoch 9/15\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8460 - acc: 0.5980 - val_loss: 0.9732 - val_acc: 0.5554\n",
            "Epoch 10/15\n",
            "15131/15131 [==============================] - 22s 1ms/step - loss: 0.8407 - acc: 0.6103 - val_loss: 0.9670 - val_acc: 0.5623\n",
            "Epoch 11/15\n",
            "15131/15131 [==============================] - 22s 1ms/step - loss: 0.8279 - acc: 0.6141 - val_loss: 0.9665 - val_acc: 0.5661\n",
            "Epoch 12/15\n",
            "15131/15131 [==============================] - 22s 1ms/step - loss: 0.8234 - acc: 0.6170 - val_loss: 0.9751 - val_acc: 0.5709\n",
            "Epoch 13/15\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8173 - acc: 0.6225 - val_loss: 0.9742 - val_acc: 0.5581\n",
            "Epoch 14/15\n",
            "15131/15131 [==============================] - 20s 1ms/step - loss: 0.8052 - acc: 0.6332 - val_loss: 0.9891 - val_acc: 0.5532\n",
            "Epoch 15/15\n",
            "15131/15131 [==============================] - 20s 1ms/step - loss: 0.7958 - acc: 0.6389 - val_loss: 0.9752 - val_acc: 0.5591\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5a47d585f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 361
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCRJkYeaTnnA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cb7edfe1-09e9-4601-e98b-27e59b576c52"
      },
      "source": [
        "model_Bi_LSTM_att111.evaluate(X_test.cpu().numpy(), y_test_final)"
      ],
      "execution_count": 362,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1869/1869 [==============================] - 3s 1ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9751780367969127, 0.5591225256060074]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 362
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "257caebf-976c-4f61-a579-a212394591ae",
        "id": "OSb82TSgTsRB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "from keras.layers import GRU\n",
        "\n",
        "model_Bi_LSTM_att2 = Sequential()\n",
        "model_Bi_LSTM_att2.add(Bidirectional(LSTM(10, dropout=0.5, recurrent_dropout=0.5, return_sequences=True), input_shape=(31, 3072), merge_mode='concat'))\n",
        "model_Bi_LSTM_att2.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "#model_Bi_LSTM_att2.add(Bidirectional(LSTM(5, dropout=0.5, recurrent_dropout=0.5), merge_mode='concat'))\n",
        "model_Bi_LSTM_att2.add(Bidirectional(GRU(8, dropout=0.5, recurrent_dropout=0.5, return_sequences=False), merge_mode='concat'))\n",
        "model_Bi_LSTM_att2.add(Dense(3, activation='softmax'))\n",
        "model_Bi_LSTM_att2.summary()\n",
        "model_Bi_LSTM_att2.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "model_Bi_LSTM_att2.fit(x=X_train.cpu().numpy(), y=y_train_final, validation_data=(X_test.cpu().numpy(), y_test_final),\tbatch_size=512, epochs=12, shuffle=True)\n",
        "\n"
      ],
      "execution_count": 369,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_19 (Bidirectio (None, 31, 20)            246640    \n",
            "_________________________________________________________________\n",
            "seq_self_attention_10 (SeqSe (None, 31, 20)            1345      \n",
            "_________________________________________________________________\n",
            "bidirectional_20 (Bidirectio (None, 16)                1392      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 249,428\n",
            "Trainable params: 249,428\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 15131 samples, validate on 1869 samples\n",
            "Epoch 1/12\n",
            "15131/15131 [==============================] - 32s 2ms/step - loss: 1.0488 - acc: 0.4369 - val_loss: 1.0420 - val_acc: 0.4361\n",
            "Epoch 2/12\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.9787 - acc: 0.4974 - val_loss: 0.9978 - val_acc: 0.4864\n",
            "Epoch 3/12\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.9332 - acc: 0.5377 - val_loss: 0.9994 - val_acc: 0.5013\n",
            "Epoch 4/12\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.9098 - acc: 0.5432 - val_loss: 0.9801 - val_acc: 0.5136\n",
            "Epoch 5/12\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8960 - acc: 0.5635 - val_loss: 0.9838 - val_acc: 0.5270\n",
            "Epoch 6/12\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8795 - acc: 0.5709 - val_loss: 0.9818 - val_acc: 0.5308\n",
            "Epoch 7/12\n",
            "15131/15131 [==============================] - 22s 1ms/step - loss: 0.8719 - acc: 0.5876 - val_loss: 0.9796 - val_acc: 0.5265\n",
            "Epoch 8/12\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8589 - acc: 0.5887 - val_loss: 0.9860 - val_acc: 0.5431\n",
            "Epoch 9/12\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8445 - acc: 0.5988 - val_loss: 0.9634 - val_acc: 0.5575\n",
            "Epoch 10/12\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8379 - acc: 0.6059 - val_loss: 0.9769 - val_acc: 0.5495\n",
            "Epoch 11/12\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8311 - acc: 0.6085 - val_loss: 0.9709 - val_acc: 0.5436\n",
            "Epoch 12/12\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8227 - acc: 0.6163 - val_loss: 0.9740 - val_acc: 0.5618\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5a2e9ca4e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 369
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNOwba_Sdfjm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4da918ec-2d54-464d-bba6-09cde03e0ec9"
      },
      "source": [
        "model_Bi_LSTM_att2.evaluate(X_test.cpu().numpy(), y_test_final)"
      ],
      "execution_count": 370,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1869/1869 [==============================] - 3s 2ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.973983029970962, 0.5617977529365535]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 370
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9d05decc-ba56-4fa7-f3ce-7e83178a22cf",
        "id": "h2zrVqvzeV_e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "from keras.layers import GRU\n",
        "\n",
        "model_Bi_LSTM_att21 = Sequential()\n",
        "model_Bi_LSTM_att21.add(Bidirectional(LSTM(10, dropout=0.5, recurrent_dropout=0.5, return_sequences=True), input_shape=(31, 3072), merge_mode='concat'))\n",
        "model_Bi_LSTM_att21.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
        "#model_Bi_LSTM_att2.add(Bidirectional(LSTM(5, dropout=0.5, recurrent_dropout=0.5), merge_mode='concat'))\n",
        "model_Bi_LSTM_att21.add(Bidirectional(GRU(8, dropout=0.5, recurrent_dropout=0.5, return_sequences=False), merge_mode='concat'))\n",
        "model_Bi_LSTM_att21.add(Dense(3, activation='softmax'))\n",
        "model_Bi_LSTM_att21.summary()\n",
        "model_Bi_LSTM_att21.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "model_Bi_LSTM_att21.fit(x=X_train.cpu().numpy(), y=y_train_final, validation_data=(X_test.cpu().numpy(), y_test_final),\tbatch_size=512, epochs=12, shuffle=True)\n",
        "\n"
      ],
      "execution_count": 373,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_25 (Bidirectio (None, 31, 20)            246640    \n",
            "_________________________________________________________________\n",
            "seq_self_attention_13 (SeqSe (None, 31, 20)            1345      \n",
            "_________________________________________________________________\n",
            "bidirectional_26 (Bidirectio (None, 16)                1392      \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 249,428\n",
            "Trainable params: 249,428\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 15131 samples, validate on 1869 samples\n",
            "Epoch 1/12\n",
            "15131/15131 [==============================] - 36s 2ms/step - loss: 1.0405 - acc: 0.4419 - val_loss: 1.0235 - val_acc: 0.4644\n",
            "Epoch 2/12\n",
            "15131/15131 [==============================] - 22s 1ms/step - loss: 0.9717 - acc: 0.5037 - val_loss: 0.9967 - val_acc: 0.4971\n",
            "Epoch 3/12\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.9309 - acc: 0.5370 - val_loss: 0.9864 - val_acc: 0.5313\n",
            "Epoch 4/12\n",
            "15131/15131 [==============================] - 22s 1ms/step - loss: 0.9076 - acc: 0.5498 - val_loss: 0.9871 - val_acc: 0.5233\n",
            "Epoch 5/12\n",
            "15131/15131 [==============================] - 22s 1ms/step - loss: 0.8903 - acc: 0.5681 - val_loss: 0.9900 - val_acc: 0.5067\n",
            "Epoch 6/12\n",
            "15131/15131 [==============================] - 22s 1ms/step - loss: 0.8795 - acc: 0.5772 - val_loss: 0.9888 - val_acc: 0.5259\n",
            "Epoch 7/12\n",
            "15131/15131 [==============================] - 23s 1ms/step - loss: 0.8730 - acc: 0.5830 - val_loss: 0.9853 - val_acc: 0.5409\n",
            "Epoch 8/12\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8566 - acc: 0.5924 - val_loss: 0.9828 - val_acc: 0.5377\n",
            "Epoch 9/12\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8485 - acc: 0.5993 - val_loss: 0.9864 - val_acc: 0.5457\n",
            "Epoch 10/12\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8402 - acc: 0.5998 - val_loss: 0.9869 - val_acc: 0.5527\n",
            "Epoch 11/12\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8283 - acc: 0.6127 - val_loss: 0.9752 - val_acc: 0.5607\n",
            "Epoch 12/12\n",
            "15131/15131 [==============================] - 21s 1ms/step - loss: 0.8229 - acc: 0.6202 - val_loss: 0.9763 - val_acc: 0.5564\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5a247f73c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 373
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRJO43Vl5GoF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "1ac6956e-919d-4762-8ba3-60cd54eaffe0"
      },
      "source": [
        ""
      ],
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-288-518985267260>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m#--------------------------------------------ENSEMBLE MODELS--------------------------------------------#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mmodel3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-288-518985267260>\u001b[0m in \u001b[0;36mreturn_model\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreturn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgood_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1_train_app\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgood_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1_train_app\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X1_train_app' is not defined"
          ]
        }
      ]
    }
  ]
}